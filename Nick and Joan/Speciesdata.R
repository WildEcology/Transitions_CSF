##  ---------------------------------------------------------------------------------------------------------------------  ##
                              # Species Distribution Modeling Code
##  ---------------------------------------------------------------------------------------------------------------------  ##
# Written by Nicholas J Lyon
  ## nicholasjlyon@gmail.com

# PURPOSE
  ## This script does the nuts and bolts of species distribution modeling (hereafter "SDM")
  ## It will pull occurrence records for a given species* and then step through the modeling process following
  ## For the generation of pre-cropped climate data, take a look at the "climate_dataprep.R" script
    ### * = use find/replace to change the 6-letter code assigned to each species and just create a folder
    ### with that code in your WD to have all relevant outputs saved in there automatically
    ### My codes are generated by taking first three letters of the genus
    ### and the first three of the specific epithet (of the synonym I'm most familiar with)

# Get necessary libraries
library(rgbif); library(rbison); #library(ecoengine);
library(spocc);
#library(rgdal);
library(rgeos); library(dismo); library(raster); #library(scrubr); 
library(geosphere);
library(scales); #library(rJava); 
library(mapr); library(ggmap); library(ggplot2);
library(sf)

librarian::shelf(scrubr)

# devtools::install_github('gbif', build_vignettes = TRUE)
# # Check to make sure maxent is running through R
# system.file('java', package = 'dismo'); maxent(); .jinit()
# 
# # Set working directory
# #setwd("~/Documents/School/1. Iowa State/_MS Project/_Leopold Project/Lyon_etal_2018_SDM_Project")

# Handy shapefiles denoting country/state/ecotone borders
# countries <- terra::ogr('./Borders', 'TM_WORLD_BORDERS-0.3')
# states <- rgdal::readOGR('./Borders', 'cb_2015_us_state_20m')
# eco <- rgdal::readOGR('./Borders', 'provinces', verbose = F)
# 
# install.packages('ecoengine')

##  ---------------------------------------------------------------------------------------------------------------------  ##
                              # Data Entry and Preliminary Cleaning
##  ---------------------------------------------------------------------------------------------------------------------  ##
# Set synonyms of the species you are interested in pulling
spnames <- c('Monarda fistulosa', 'Monarda menthifolia')

#librarian::shelf("spocc")

# Set a bounding box to collect records wtihin (see "occ" function in "spocc" package)
df_mult <- occ(query = spnames, from = c('gbif', "ecoengine", "bison"), limit = 100,
               geometry = c(-140, 22, -58, 55), has_coords = T) ## set limit to 10,000 to get all data point



# "Fix" the species names to match the entries provided in the "spnames" vector
df_nam <- fixnames(df_mult, how = "query")

# And get the tibbles into a single dataframe
df_comb <- occ2df(df_nam)

# Get rid of records with impossible geo-references or ones where the reference is missing
records_geoclean <- df_comb %>% coord_impossible(drop = T) %>% coord_incomplete(drop = T) %>% coord_unlikely(drop = T)

# Get rid of records that are not appropriately date referenced
records_dateclean <- records_geoclean
records_dateclean <- date_standardize(records_dateclean, "%Y")
records_dateclean <- date_missing(records_dateclean, format = "%Y", drop = T)
records_dateclean$date <- as.numeric(records_dateclean$date)

# And ditch records that are from outside of the training climate range
records <- subset(records_dateclean, records_dateclean$date >= 1960 & records_dateclean$date <= 1990)

# How many does that leave us with?
nrow(records)

# Get a spatial dataframe for climate cropping
recordsSpatial <- SpatialPointsDataFrame(coords = cbind(records$longitude, records$latitude), data = records,
                                         proj4string = CRS('+proj=longlat +datumWGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0'))

# Save it!
saveRDS(records, './Species Records/MONFIS V1 - Initial Records Pull.rds')

##  ----------------------------------------------------------------------------------------------------------------------  ##
                          # Match Records with Environmental Data
##  ----------------------------------------------------------------------------------------------------------------------  ##
# Get a single object of all the current climate data cropped by "climate_dataprep.R" (& elevation info)
env <- stack(c('./WORLDCLIM Data/Cropped Elevation/elevation.tif',
               list.files('./WORLDCLIM Data/Cropped 1960-1990', full.names = T)))

# Bind the climate Raster Stack to your occurrence records
envSpecies <- data.frame(extract(env, cbind(records$longitude, records$latitude)))
records <- cbind(records, envSpecies)

# Ditch NAs
if (any(is.na(rowSums(envSpecies)))) records <- records[-which(is.na(rowSums(envSpecies))), ]

# Save this composite object
saveRDS(records, './Species Records/MONFIS V2 - Records & Env.rds')

##  ----------------------------------------------------------------------------------------------------------------------  ##
                                    # Predictor Selection
##  ----------------------------------------------------------------------------------------------------------------------  ##
# Get a new climate stack object (similar to "env" object earlier, but this one includes the whole landscape)
climate <- stack(c('./WORLDCLIM Data/Cropped Elevation/elevation.tif',
               list.files('./WORLDCLIM Data/Cropped 1960-1990', full.names = T)))

# Set candidate variables for inclusion in SDM
candidates <- c('WC08', 'WC09', 'WC10', 'WC11', 'WC16', 'WC17', 'WC18', 'WC19')

# What are the correlations among variables
par(mfrow = c(1, 1))
correl <- cor(records[ , candidates], method = 'pearson')
pos <- correl > 0.7; neg <- correl < -0.7
spokePlot(pos = pos, neg = neg, lwdPos = 2, lwdNeg = 2, colPos = 'black', colNeg = 'red')

# Include only non-correlated variables as predictor variables
predictors <- c('WC08', 'WC09', 'WC10', 'WC17')

##  ---------------------------------------------------------------------------------------------------------------------  ##
                                # Random BG Site Selection
##  ---------------------------------------------------------------------------------------------------------------------  ##
# In order to test these sorts of model, one must compare predictive ability at random or targeted BG sites
# For sample size-limited species, random can be a more logistically feasible method
randomBgSites <- randomPoints(climate, 10000)

randomBgEnv <- as.data.frame(extract(climate, randomBgSites))

isNa <- is.na(rowSums(randomBgEnv))
if (any(isNa)) {
  randomBgSites <- randomBgSites[-which(isNa), ]
  randomBgEnv <- randomBgEnv[-which(isNa), ]}

randomBg <- cbind(randomBgSites, randomBgEnv)
names(randomBg)[1:2] <- c('longitude', 'latitude')

save(randomBg, file = './Species Records/MONFIS - Random BG.Rdata', compress = T)

trainData <- rbind(records[ , c(paste0('WC0', 1:9), paste0('WC', 10:19))],
                   randomBgEnv[ , c(paste0('WC0', 1:9), paste0('WC', 10:19))])
presBg <- c(rep(1, nrow(records)), rep(0, nrow(randomBgEnv)))

# Generate training data
trainData <- rbind(records[ , predictors], randomBg[ , predictors])
presBg <- c(rep(1, nrow(records)), rep(0, nrow(randomBg)))

# Create a model in MaxEnt that includes both your training data and the knowledge of which points are
  # random or actual presences
manualSelectModel <- maxent(x = trainData, p = presBg)

# Save that model
save(manualSelectModel, file = './Preliminary Models/MONFIS Model V1 - Manual Predictors.Rdata', compress = T)

# Predict the model across the landscape
manualSelectMap <- predict(manualSelectModel, climate,
                           filename = './Preliminary Models/MONFIS Map V1 - Manual Predictors.Rdata',
                           format = 'GTiff', overwrite = T)

# Plot just so you can get a rough sense of the data
plot(manualSelectMap, main = 'Predictor Variables Only')
sp::plot(countries, add = T, border = 'gray45')
sp::plot(states, add = T, border = 'gray45')

##  ---------------------------------------------------------------------------------------------------------------------  ##
                              # Thinning in Environmental Space
##  ---------------------------------------------------------------------------------------------------------------------  ##
# Run PCA on Environmental variables
pca <- princomp(randomBg[ , predictors], cor = T)

# Plot
par(mfrow = c(1, 2))
plot(pca, main = 'Variances explained by axes', cex.main = 0.7)
biplot(pca, main = 'Biplot', cex = 0.6)
par(mfrow = c(1, 1))

# Rotate (predict) environment at species' presences into PCA space
speciesPca <- predict(pca, newdata = records[ , predictors]); par(mfrow = c(1, 1))

# Plot
par(pty='s')
plot(pca$scores[ , 1:2], main = 'Biplot')

numBins <- 30
for (x in seq(min(pca$scores[ , 1]), max(pca$scores[ , 1]), length.out = numBins + 1)) {
  abline(v = x, lwd = 0.5, col = alpha('black', 0.3))
  for (y in seq(min(pca$scores[ , 2]), max(pca$scores[ , 2]), length.out=numBins + 1)) {
    abline(h = y, lwd = 0.5, col = alpha('black', 0.3))}}

points(speciesPca[ , 1:2], pch = 21, bg = 'mediumseagreen')
legend('bottomleft', legend = c('Target BG', 'Presence'), col = c('black', 'mediumseagreen'), pch = c(1, 16))

# Matrix to store number of points in each Environmental "cell"
env <- matrix(rep(0, 40^2), nrow = 40)

# Calculate bin boundaries
pc1Bins <- seq(min(pca$scores[ , 1]) - 0.00001, max(pca$scores[ , 1]), length.out = numBins + 1)
pc2Bins <- seq(min(pca$scores[ , 2]) - 0.00001, max(pca$scores[ , 2]), length.out = numBins + 1)

# Data frame for new records
envThinnedRecords <- data.frame()

# Find records in each Environmental cell and randomly select one
for (pc1 in 1:numBins) {
  for (pc2 in 1:numBins) {
    
    recordsInCell <- which(speciesPca[ , 1] > pc1Bins[pc1] &
                             speciesPca[ , 1] <= pc1Bins[pc1 + 1] & speciesPca[ , 2] > pc2Bins[pc2] &
                             speciesPca[ , 2] <= pc2Bins[pc2 + 1])
    ## Get index of record(s) in this cell
    
    if (length(recordsInCell) > 0){
      chosenRecord<-if(length(recordsInCell) == 1){
        recordsInCell} else {sample(recordsInCell, 1)}
      envThinnedRecords <- rbind(envThinnedRecords, records[chosenRecord, ])}}}
## If there is >1 record, select one

# Predict PCA scores at thinned presence sites
thinnedPca <- predict(pca, envThinnedRecords)

# Plot
par(pty = 's')
plot(pca$scores[ , 1:2], main = 'Biplot')

for (x in seq(min(pca$scores[ , 1]), max(pca$scores[ , 1]), length.out = numBins + 1)) { 
  abline(v = x, lwd = 0.5, col = alpha('black', 0.3))
  for (y in seq(min(pca$scores[ , 2]), max(pca$scores[ , 2]), length.out = numBins + 1)) {
    abline(h = y, lwd = 0.5, col = alpha('black', 0.3))}}

points(speciesPca[ , 1:2], pch = 21, bg = 'mediumseagreen')
points(thinnedPca[ , 1:2], pch = 23, bg = 'red', cex = 1)

legend('bottomleft', legend = c('Background', paste0('Unthinned n=', nrow(records)),
       paste0('Thinned n=', nrow(envThinnedRecords))),
       col = c('black', 'mediumseagreen', 'red'),
       pch = c(1, 16, 17), bg = 'white')

# Make training data frame with predictors and vector of 1/0 for presence/background
trainData <- rbind(envThinnedRecords[ , predictors], randomBg[ , predictors])

presBg <- c(rep(1, nrow(envThinnedRecords)), rep(0, nrow(randomBg)))

# Model species
envThinModel <- maxent(x = trainData, p = presBg)

# Save!
save(envThinModel, file = './Preliminary Models/MONFIS Model V2 - Env Thinned.Rdata', compress = T)

# Write current raster
envThinMap <- predict(envThinModel, climate, 
                      filename = './Preliminary Models/MONFIS Map V2 - Env Thinned',
                      format = 'GTiff', overwrite = T)

# Plot
par(mfrow = c(1, 1))
plot(envThinMap, main = 'Thinned Map')
sp::plot(countries, add = T, border = 'gray45')
sp::plot(states, add = T, border = 'gray45')

##  ---------------------------------------------------------------------------------------------------------------------  ##
                                  # Model Tuning & Smoothing
##  ---------------------------------------------------------------------------------------------------------------------  ##
# Need to set the regularization parameter, beta
# This determines how strict of a suitability penalty to apply to climates outside of the training range
  ## I.e. novel climates
# Can use delta AIC to assess this
baselineModel <- maxentAic(trainData = trainData, presentBg = presBg, betaToTest = c(0.5, 1:5),
                           params = c('linear=true', 'quadratic=true', 'product=false', 
                                      'threshold=false', 'hinge=false'), verbose = T)

# Get the one with the lowest AIC only
baselineModel <- baselineModel$model

# Save (this is the core model you'll use from here on out.)
save(baselineModel, file = './Preliminary Models/MONFIS Model V3 - Baseline.Rdata', compress = T)

# And get a map of it
baselineMap <- predict(baselineModel, climate,
                       filename = './Preliminary Models/MONFIS Map V3 - Baseline',
                       format = 'GTiff', overwrite = T)

# And also plot it in a save function
jpeg(filename = './Maps/Current Maps/MONFIS Current.jpg', width = 1080, height = 1138, quality = 100)
plot(baselineMap, main = 'MONFIS Complete')
sp::plot(countries, add = T, border = 'gray45')
sp::plot(states, add = T, border = 'gray45')
dev.off()

# Just for fun, where are your chosen records from?
jpeg(filename = './Maps/Current Maps/MONFIS Occ.jpg', width = 1080, height = 1138, quality = 100)
plot(recordsSpatial, main = 'MONFIS')
sp::plot(countries, add = T, col = 'gray80')
sp::plot(states, add = T, col = 'gray80')
points(records$longitude, records$latitude, pch = 21, bg = 'gray45')
points(envThinnedRecords$longitude, envThinnedRecords$latitude, pch = 21, bg = 'mediumseagreen')
legend('bottomleft', legend = c(paste0('Full n = ', nrow(records)),
                                paste0('Thinned n = ', nrow(envThinnedRecords))), bty = 'n')
dev.off()

##  ---------------------------------------------------------------------------------------------------------------------  ##
                  # Threshold-Indepedent Metrics (TIMs) of Model Evaluation
##  ---------------------------------------------------------------------------------------------------------------------  ##
# Draw in objects to test model discernment ability
predPres <- extract(baselineMap, cbind(records$longitude, records$latitude))
predBg <- extract(baselineMap, cbind(randomBg$longitude, randomBg$latitude))

# Area under the receiver-operator characteristic curve (AUC)
presDistrib <- hist(predPres, plot = F, breaks = seq(0, 1, length.out = 21))$counts
bgDistrib <- hist(predBg, plot = F, breaks = seq(0, 1, length.out = 21))$counts
  ## Tally frequency of predictions at presence and background sites in bins

# Convert to proportion of sites
presDistrib <- presDistrib / sum(presDistrib)
bgDistrib <- bgDistrib / sum(bgDistrib)

# Plot distribution of predictions at presences and background sites
labels <- round(seq(0, 1, length.out = 21), 2)
barplot(bgDistrib, col = alpha('red', 0.5), names.arg = labels[2:21],
        ylab = 'Proportion of Sites', xlab='Prediction', main = 'Distribution of Predictions')
barplot(presDistrib, col = alpha('forestgreen', 0.5), add = T)

abline(v = c(2, 8, 17), lwd = 3)
text(x = c(2, 8, 17) + 1, y = c(0.2, 0.2, 0.2), labels = LETTERS[1:3], cex = 1.4)

# Will store sensitivity and false positive rate
sensitivity <- FPR <- numeric()

# Use 21 threshold values from 0 to 1
for (threshold in seq(0, 1, length.out=21)) {
  thisSens <- sum(predPres >= threshold) / length(predPres)
  thisFPR <- sum(predBg >= threshold) / length(predBg)
  sensitivity <- c(sensitivity, thisSens)
  FPR <- c(FPR, thisFPR)}

# Plot ROC (receiver-operator characteristic) curve
plot(FPR, sensitivity, type = 'l', xlab = 'False positive rate (=1 - specificity)', 
     ylab = 'True positive rate (= sensitivity)', main = 'ROC Curve', lwd = 2, col = 'blue')
abline(a = 0, b=1, lty = 'dashed')

points(c(FPR[which.min(abs(seq(0, 1, length.out=21) - 0.12))],
         FPR[which.min(abs(seq(0, 1, length.out=21) - 0.35))],
         FPR[which.min(abs(seq(0, 1, length.out=21) - 0.70))]),
       c(sensitivity[which.min(abs(seq(0, 1, length.out=21) - 0.12))],
         sensitivity[which.min(abs(seq(0, 1, length.out=21) - 0.35))],
         sensitivity[which.min(abs(seq(0, 1, length.out=21) - 0.70))]), pch = 16)

text(c(FPR[which.min(abs(seq(0, 1, length.out=21) - 0.12))],
       FPR[which.min(abs(seq(0, 1, length.out=21) - 0.35))],
       FPR[which.min(abs(seq(0, 1, length.out=21) - 0.70))]) + 0.05,
     c(sensitivity[which.min(abs(seq(0, 1, length.out=21) - 0.12))],
       sensitivity[which.min(abs(seq(0, 1, length.out=21) - 0.35))],
       sensitivity[which.min(abs(seq(0, 1, length.out=21) - 0.70))]),
     labels=LETTERS[1:3], cex=1.4)

##  ---------------------------------------------------------------------------------------------------------------------  ##
                            # Model Validation via Random k Folds
##  ---------------------------------------------------------------------------------------------------------------------  ##
# K fold-based model validation is actually quite simple
# You use 1/kth (in this case 1/5th) of the data and train a new model with only those points
# Then you see how well this purposely limited model predicts the remaining k-1/kths (4/5ths) of presences
kPres <- kfold(x = records, k = 5)
kBg <- kfold(x = randomBg, k = 5)

# Good to get a visual of what this can look like
plot(recordsSpatial, main = 'k-fold #1')
sp::plot(countries, add = T, col = 'gray80')
sp::plot(states, add = T, col = 'gray80')
points(records$longitude, records$latitude)
points(records$longitude[kPres == 1], records$latitude[kPres == 1], bg = 'red', pch = 21)

# Now do it
aucRandom <- cbiRandom <- numeric()
for (i in 1:5) {
  cat('K-fold', i); flush.console()
  
  trainData <- rbind(records[kPres!=i, predictors], randomBg[kBg!=i, predictors])
  ## Make training data frame with predictors and vector of 1/0 for presence/background
  
  presBg <- c(rep(1, sum(kPres!=i)), rep(0, sum(kBg!=i)))
  
  model <- maxentAic(trainData = trainData, presentBg = presBg, betaToTest = c(0.5, 1:5),
                     params = c('linear=true', 'quadratic=true', 
                                'product=false', 'threshold=false', 'hinge=false'))
  
  model <- model$model
  
  save(model, file = paste0('./Preliminary Models/All Spp - K Folds/MONFIS - K fold ', i, '.Rdata'),
       compress = T)
  
  # Predict to presences and background sites
  predPres <- raster::predict(model, x=records[kPres==i, ])
  predBg <- raster::predict(model, x=randomBg[kBg==i, ])
  
  thisEval <- evaluate(p=as.vector(predPres), a=as.vector(predBg))
  thisAuc <- thisEval@auc
  cat(': AUC =', round(thisAuc, 3), '\n')
  flush.console()
  aucRandom <- c(aucRandom, thisAuc)
}

# And you report this mean AUC as a metric for model performance
print(paste('Mean AUC:', round(mean(aucRandom), 3)))

# Predict to presences and background sites of non-k-folded sites
predPres <- raster::predict(baselineModel, x = records)
predBg <- raster::predict(baselineModel, x = randomBg)

thisEval <- evaluate(p = as.vector(predPres), a = as.vector(predBg))
aucResub <- thisEval@auc

# Plot
par(mfrow = c(1,1))
barplot(c(aucResub, mean(aucRandom)), names.arg = c('resubstituted', 'k-fold'), 
        cex.lab = 0.7, col = 'gray', ylab = 'AUC', main = 'AUC')

##  ---------------------------------------------------------------------------------------------------------------------  ##
                                  # CCSM4 2050 @ RCP 4.5
##  ---------------------------------------------------------------------------------------------------------------------  ##
# Get the pre-cropped climate data and project the model into those conditions
cc45 <- stack(c('./WORLDCLIM Data/Cropped Elevation/elevation.tif',
               list.files('./WORLDCLIM Data/Cropped CC 45', full.names = T)))

# And subset based on the predictors created earlier
cc45.pred <- subset(cc45, predictors)

# Predict your beautiful model in these novel conditions
future45BaselineMap <- predict(baselineModel, cc45.pred, 
                               filename = './Future Models/MONFIS - CC 45',
                               format = 'GTiff', overwrite = T)

# Plot it
jpeg(filename = './Maps/Future Maps/MONFIS CC 45.jpg', width = 1080, height = 1138, quality = 100)
plot(future45BaselineMap, main = 'MONFIS CC 4.5')
sp::plot(countries, add = T, border = 'gray45')
sp::plot(states, add = T, border = 'gray45')
dev.off()

# MESS (Multivariate Environmental Similarity Surface) Map
extrapMap45 <- mess(x = cc45.pred, v = randomBg[ , predictors], full = F)

# Save once it is generated
jpeg(filename = './Maps/MESS Maps/MONFIS CC 45 MESS.jpg', width = 1080, height = 1138, quality = 100)
plot(extrapMap45, main = 'MONFIS MESS CC 4.5')
sp::plot(countries, add = T)
sp::plot(states, add = T)
dev.off()
  ## Greater negative ( - ) values indicate greater extrapolation into that area
  ## Positive ( + ) values indicate areas where no extraploation was necessary

##  ---------------------------------------------------------------------------------------------------------------------  ##
                              # CCSM4 2050 @ RCP 8.5
##  ---------------------------------------------------------------------------------------------------------------------  ##
# Get the pre-cropped climate data and project the model into those conditions
cc85 <- stack(c('./WORLDCLIM Data/Cropped Elevation/elevation.tif',
                list.files('./WORLDCLIM Data/Cropped CC 85', full.names = T)))

# And subset based on the predictors created earlier
cc85.pred <- subset(cc85, predictors)

# Predict your beautiful model in these novel conditions
future85BaselineMap <- predict(baselineModel, cc85.pred, 
                               filename = './Future Models/MONFIS - CC 85',
                               format = 'GTiff', overwrite = T)

# Plot it
jpeg(filename = './Maps/Future Maps/MONFIS CC 85.jpg', width = 1080, height = 1138, quality = 100)
plot(future85BaselineMap, main = 'MONFIS CC 8.5')
sp::plot(countries, add = T, border = 'gray45')
sp::plot(states, add = T, border = 'gray45')
dev.off()

# MESS (Multivariate Environmental Similarity Surface) Map
extrapMap85 <- mess(x = cc85.pred, v = randomBg[ , predictors], full = F)

# Save once it is generated
jpeg(filename = './Maps/MESS Maps/MONFIS CC 85 MESS.jpg', width = 1080, height = 1138, quality = 100)
plot(extrapMap85, main = 'MONFIS MESS CC 8.5')
sp::plot(countries, add = T)
sp::plot(states, add = T)
dev.off()

